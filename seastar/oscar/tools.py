# -*- coding: utf-8 -*-
"""Functions for the processing of OSCAR airbone data."""
import os
import numpy as np
import re
from seastar.utils.readers import readNetCDFFile
from seastar.utils.tools import list_duplicates
from scipy import interpolate
import xarray as xr
from datetime import datetime as dt
import re
from _version import __version__
from _logger import logger


def load_L1AP_OSCAR_data(file_path, file_list):
    """
    Load OSCAR data.

    Loads in OSCAR Fore/Mid/Aft antenna files from the path designated in
    `file_path` and files in `file_list`. Returns data in a dict of
    `{file index : xarray.Dataset}`

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR data files
    file_list : ``list`` of ``str``
        List of file name of the Fore/Mid/Aft antenna data triplet

    Returns
    -------
    ds_dict : ``dict``, ``xarray.Dataset``
        OSCAR Fore/Mid/Aft antenna data as ``xarray.Dataset`` in a dict with
        keys as the name of each antenna as a ``str``

    """
    ds_dict = dict()
    for ind, file_name in enumerate(file_list):
        antenna = identify_antenna_location_from_filename(file_name)
        ds_dict[antenna] = readNetCDFFile(os.path.join(file_path,file_name))
    return ds_dict


def identify_antenna_location(ds):
    """
    Identify ATI antenna location.

    Identifies the antenna direction in an OSCAR dataset by interrogating the
    minimum and maximum processed Doppler values.

    Parameters
    ----------
    ds : ``xarray.Dataset``
        OSCAR dataset containing MinProcessedDoppler and MaxProcessedDoppler
        variables

    Returns
    -------
    antenna_location : ``str``
        Antenna location ('Fore', 'Aft', 'Mid')

    """
    doppler_mean = np.mean([ds.MinProcessedDoppler, ds.MaxProcessedDoppler])
    if np.abs(doppler_mean) < 500:
        antenna_location = 'Mid'
    elif doppler_mean < 0:
        antenna_location = 'Aft'
    elif doppler_mean > 0:
        antenna_location = 'Fore'

    return antenna_location

def find_file_triplets(file_path):
    """
    Find file list indices of matching antenna files.

    Sorts a file list generated by `os.listdir(file_path)` and creates a list
    `file_time_triplets` of file aquisition times (YYYYMMDD'T'HHMMSS) along the
    1st column and the corresponding indices of the Fore/Mid/Aft antenna
    data files associated with each aquisition time in the 2nd column.

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR NetCDF files

    Returns
    -------
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list
        generated by `os.listdir(file_path)`
        
    """
    file_list = sorted(os.listdir(file_path))
    num_files = len(file_list)
    r = re.compile("[0-9]{8}T[0-9]{6}")
    file_time = list()
    for file in range(num_files):
        file_name_split = str.split(file_list[file], '_')
        file_time.append(
            file_name_split[
                file_name_split.index(
                    list(filter(r.match, file_name_split))[0]
                    )
                ]
            )
    file_time_triplets = sorted(list_duplicates(file_time))
    return file_time_triplets


def antenna_idents(ds):
    """
    Build antenna identifier list.

    Generates list of antenna identifiers of the form ['Fore','Mid','Aft'] to
    correspond with keys in the dataset dictionary generated by
    `seastar.utils.readers.load_OSCAR_data`

    Parameters
    ----------
    ds : ``dict``
        OSCAR data stored as a dict with antenna number as keys and loaded
        data in ``xarray.Dataset`` format as values

    Returns
    -------
    antenna_list : ``list``
        List of antenna identifiers in the form ['Fore', 'Mid', 'Aft'],
        corresponding to the data and keys stored in `ds`

    """
    antenna_list = list()
    for i in list(ds.keys()):
        antenna_list.append(identify_antenna_location(ds[i]))
    return antenna_list


def identify_antenna_location_from_filename(file_name):
    """
    Identify antenna from filename.

    Parameters
    ----------
    file_list : ``str``
        List of files of data aquisition for the Fore/Mid/Aft antenna triplet.

    Returns
    -------
    antenna : ``str``
        Name of antenna as strings like 'Mid', 'Fore' or 'Aft'.

    """
    r = re.compile(r'(?:\D*\d\D*\d\D*)$')
    antenna_filter = filter(r.match, file_name.split('_'))
    antenna_number = "".join(map(str, antenna_filter))
    antenna_identifiers = {'0': 'Mid', '3': 'Fore', '7': 'Aft'}
    antenna = "".join({antenna_identifiers[i] for i in antenna_number if i in antenna_identifiers})
    return antenna


def colocate_variable_lat_lon(data_in, latitude, longitude, ds_out):
    """
    Co-locate data by lat/lon coordinates.

    Co-locates data from once set of lat/lon coordinates to another.

    Parameters
    ----------
    data_in : `xr.DataArray`, `array`
        Data at points to be co-locoated
    latitude : `array` of `float`
        Array of latitude coordinates
    longitude : `array` of `float`
        Array of longitude coordinates
    ds_out : `xr.DataArray`
        `xr.DataArray` with lat and lon coordinates to co-locate the data to.

    Returns
    -------
    colocated_var : `xr.DataArray`
        Data array of co-located data

    """
    new_data = interpolate.griddata(
                            points=(np.ravel(longitude),
                                    np.ravel(latitude)),
                            values=(np.ravel(data_in)),
                            xi=(ds_out.longitude.values,
                                ds_out.latitude.values),
                            )
    colocated_var = xr.DataArray(
                        data=new_data,
                        dims=ds_out.dims,
                        coords=ds_out.coords
                        )
    return colocated_var



def formatting_filename(ds):
    """
    Formatting the filename of the dataset.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to format and to save as a NetCDF file.
       
    Returns
    ----------
    ds : xr.Dataset
        The dataset with updated metadata.
    filename : `str`
        Name of the OSCAR NetCDF file.
    """    
    # Checking dataset attributes
    ds_L1B = check_attrs_dataset(ds_L1B)
    
    # Construct the filename
    date_filename = ds.attrs.get("StartTime") + '-' + ds.attrs.get("EndTime").split("T")[1]

    filename_parts = [
        date_filename,  # Already formatted as per your logic
        ds.attrs.get("Platform"),
        ds.attrs.get("ProcessingLevel"),
        ds.attrs.get("Track"),
        "{crossRes:03d}x{groundRes:03d}m".format(  
                                                 crossRes=int(ds.attrs.get("MultiLookCrossRangeEffectiveResolution", 0)),
                                                 groundRes=int(ds.attrs.get("MultiLookGroundRangeEffectiveResolution", 0))),
        ds.attrs.get("L2Processor", ""),  # Only for L2
        ds.attrs.get("DopplerGMF", ""),  # Only for L2
        f"Kp{ds.attrs.get("Kp")}" if ds.attrs.get("Kp") else "",  # Only for L2
        f"RSV{ds.attrs.get("RSVNoise")}" if ds.attrs.get("RSVNoise") else "",  # Only for L2
        __version__,
    ]

    filename = "_".join(filter(None, filename_parts)) + ".nc"

    return filename
    
    
def check_attrs_dataset(ds):
    """
    Check dataset attributes.
    Test the dataset to check if all the attributes, from the defined list of mandatory atributes, are reported in the dataset.
    It test L1 and L2 datasets.

    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to check.

    Returns
    ----------
    ds : xr.Dataset
        The dataset.
    """
    
    logger.info("Checking the attrs dataset.")

    # Check if ds is a valid xarray Dataset
    if not isinstance(ds, xr.Dataset):
        logger.error("Input 'ds' must be an xarray.Dataset.")

    # Ensure processing_level is valid
    processing_level = ds.attrs.get("ProcessingLevel")
    valid_levels = {"L1AP", "L1B", "L1C", "L2"}
    if processing_level not in valid_levels:
        logger.error(f"Invalid processing level: {processing_level}. Must be one of {valid_levels}.")
    
    # List of the mandatory attributes for L1 dataset
    required_attrs = ["Campaign", 
                    "Platform", 
                    "Track", 
                    "StartTime", 
                    "EndTime", 
                    "ProcessingLevel", 
                    "Codebase", 
                    "Repository", 
                    "CodeVersion", 
                    "DataVersion", 
                    "Comments"]
    
    # Check missing attributes
    missing_attrs = [attr for attr in required_attrs if attr not in ds.attrs]
    
    # Extend the list of attributes for every dataset level
    if processing_level == 'L1B':
        required_attrs_L1B = ["MultiLookCrossRangeEffectiveResolution", "MultiLookGroundRangeEffectiveResolution"]
        missing_attrs.extend([attr for attr in required_attrs_L1B if attr not in ds.attrs])
    if processing_level == "L1C":
        required_attrs_L1C = ["Calibration", "CalibrationResolution", "NRCSGMF"]
        missing_attrs.extend([attr for attr in required_attrs_L1C if attr not in ds.attrs])
    if processing_level == "L2":
        required_attrs_L2 = ["DopplerGMF", "Kp", "RSVNoise", "L2Processor"]
        missing_attrs.extend([attr for attr in required_attrs_L2 if attr not in ds.attrs])

    # Raise error if missing attributes
    if missing_attrs:
        logger.error(f"The following attrs are missing in ds.attrs: {missing_attrs}")
    else:
        logger.info("All the attributes are reported in the dataset.")

    return ds

def clean_units_attribute(ds):
    """
    Clean units attributes from dataset.
    
    Looks for global and varible `units` attributes and removes all instances
    of square brackets around the `units` value.
    
    e.g., '[m]' --> 'm'
    
    Parameters
    ----------
    ds : ``xr.dataset``
        Input dataset to clean

    Returns
    -------
    ds : ``xr.dataset``
        Dataset with cleaned units attributes.
    """
    
    def remove_brackets(unit_str):
        return re.sub(r'[\[\]]', '', unit_str)
    
    
    # Check global attributes
    if 'units' in ds.attrs and isinstance(ds.attrs['units'], str):
        ds.attrs['units'] = remove_brackets(ds.attrs['units'])
    
    # Check each variable's attributes
    for var_name, var in ds.variables.items():
        if 'units' in var.attrs and isinstance(var.attrs['units'], str):
            var.attrs['units'] = remove_brackets(var.attrs['units'])
    
    return ds


def is_valid_acq_date(acq_date):
    """Check acquisition date.
    Check if the input acquisition date is a valid date string in 'YYYYMMDD' format.

    Parameters
    ----------
        acq_date (_type_): ``str``
            Acquisition date string to validate.

    Returns
    -------
        bool
            True if the date is valid and correctly formatted (YYYYMMDD), False otherwise.
    """
    try:
        dt.strptime(acq_date, "%Y%m%d")
        return bool(re.fullmatch(r"\d{8}", acq_date))
    except ValueError:
        return False