# -*- coding: utf-8 -*-
"""Functions for the processing of OSCAR airbone data."""
import os
import numpy as np
import re
from seastar.utils.readers import readNetCDFFile
from seastar.utils.tools import list_duplicates
from scipy import interpolate
import xarray as xr
from datetime import datetime as dt
from datetime import timezone
import re
from _version import __version__
from _logger import logger


def load_L1AP_OSCAR_data(file_path, file_list):
    """
    Load OSCAR data.

    Loads in OSCAR Fore/Mid/Aft antenna files from the path designated in
    `file_path` and files in `file_list`. Returns data in a dict of
    `{file index : xarray.Dataset}`

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR data files
    file_list : ``list`` of ``str``
        List of file name of the Fore/Mid/Aft antenna data triplet

    Returns
    -------
    ds_dict : ``dict``, ``xarray.Dataset``
        OSCAR Fore/Mid/Aft antenna data as ``xarray.Dataset`` in a dict with
        keys as the name of each antenna as a ``str``

    """
    ds_dict = dict()
    for ind, file_name in enumerate(file_list):
        antenna = identify_antenna_location_from_filename(file_name)
        ds_dict[antenna] = readNetCDFFile(os.path.join(file_path,file_name))
    return ds_dict


def identify_antenna_location(ds):
    """
    Identify ATI antenna location.

    Identifies the antenna direction in an OSCAR dataset by interrogating the
    minimum and maximum processed Doppler values.

    Parameters
    ----------
    ds : ``xarray.Dataset``
        OSCAR dataset containing MinProcessedDoppler and MaxProcessedDoppler
        variables

    Returns
    -------
    antenna_location : ``str``
        Antenna location ('Fore', 'Aft', 'Mid')

    """
    doppler_mean = np.mean([ds.MinProcessedDoppler, ds.MaxProcessedDoppler])
    if np.abs(doppler_mean) < 500:
        antenna_location = 'Mid'
    elif doppler_mean < 0:
        antenna_location = 'Aft'
    elif doppler_mean > 0:
        antenna_location = 'Fore'

    return antenna_location

def find_file_triplets(file_path):
    """
    Find file list indices of matching antenna files.

    Sorts a file list generated by `os.listdir(file_path)` and creates a list
    `file_time_triplets` of file aquisition times (YYYYMMDD'T'HHMMSS) along the
    1st column and the corresponding indices of the Fore/Mid/Aft antenna
    data files associated with each aquisition time in the 2nd column.

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR NetCDF files

    Returns
    -------
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list
        generated by `os.listdir(file_path)`
        
    """
    file_list = sorted(os.listdir(file_path))
    num_files = len(file_list)
    r = re.compile("[0-9]{8}T[0-9]{6}")
    file_time = list()
    for file in range(num_files):
        file_name_split = str.split(file_list[file], '_')
        file_time.append(
            file_name_split[
                file_name_split.index(
                    list(filter(r.match, file_name_split))[0]
                    )
                ]
            )
    file_time_triplets = sorted(list_duplicates(file_time))
    return file_time_triplets


def antenna_idents(ds):
    """
    Build antenna identifier list.

    Generates list of antenna identifiers of the form ['Fore','Mid','Aft'] to
    correspond with keys in the dataset dictionary generated by
    `seastar.utils.readers.load_OSCAR_data`

    Parameters
    ----------
    ds : ``dict``
        OSCAR data stored as a dict with antenna number as keys and loaded
        data in ``xarray.Dataset`` format as values

    Returns
    -------
    antenna_list : ``list``
        List of antenna identifiers in the form ['Fore', 'Mid', 'Aft'],
        corresponding to the data and keys stored in `ds`

    """
    antenna_list = list()
    for i in list(ds.keys()):
        antenna_list.append(identify_antenna_location(ds[i]))
    return antenna_list


def identify_antenna_location_from_filename(file_name):
    """
    Identify antenna from filename.

    Parameters
    ----------
    file_list : ``str``
        List of files of data aquisition for the Fore/Mid/Aft antenna triplet.

    Returns
    -------
    antenna : ``str``
        Name of antenna as strings like 'Mid', 'Fore' or 'Aft'.

    """
    r = re.compile(r'(?:\D*\d\D*\d\D*)$')
    antenna_filter = filter(r.match, file_name.split('_'))
    antenna_number = "".join(map(str, antenna_filter))
    antenna_identifiers = {'0': 'Mid', '3': 'Fore', '7': 'Aft'}
    antenna = "".join({antenna_identifiers[i] for i in antenna_number if i in antenna_identifiers})
    return antenna


def colocate_variable_lat_lon(data_in, latitude, longitude, ds_out):
    """
    Co-locate data by lat/lon coordinates.

    Co-locates data from once set of lat/lon coordinates to another.

    Parameters
    ----------
    data_in : `xr.DataArray`, `array`
        Data at points to be co-locoated
    latitude : `array` of `float`
        Array of latitude coordinates
    longitude : `array` of `float`
        Array of longitude coordinates
    ds_out : `xr.DataArray`
        `xr.DataArray` with lat and lon coordinates to co-locate the data to.

    Returns
    -------
    colocated_var : `xr.DataArray`
        Data array of co-located data

    """
    new_data = interpolate.griddata(
                            points=(np.ravel(longitude),
                                    np.ravel(latitude)),
                            values=(np.ravel(data_in)),
                            xi=(ds_out.longitude.values,
                                ds_out.latitude.values),
                            )
    colocated_var = xr.DataArray(
                        data=new_data,
                        dims=ds_out.dims,
                        coords=ds_out.coords
                        )
    return colocated_var



def formatting_filename(ds):
    """
    Formatting the filename of the dataset.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to format and to save as a NetCDF file.
       
    Returns
    ----------
    ds : xr.Dataset
        The dataset with updated metadata.
    filename : `str`
        Name of the OSCAR NetCDF file.
    """    
    # Checking dataset attributes
    ds = check_attrs_dataset(ds)
    
    # Construct the filename
    date_filename = ds.attrs.get("StartTime") + '-' + ds.attrs.get("EndTime").split("T")[1]

    filename_parts = [
        date_filename,  # Already formatted as per your logic
        ds.attrs.get("Platform"),
        ds.attrs.get("ProcessingLevel"),
        ds.attrs.get("Track"),
        "Grd{crossRes:03d}x{groundRes:03d}m".format(  
                                                 crossRes=int(ds.attrs.get("SingleLookCrossRangeGridResolution", 0)),
                                                 groundRes=int(ds.attrs.get("SingleLookGroundRangeGridResolution", 0))),
        "Eff{crossRes:03d}x{groundRes:03d}m".format(  
                                                 crossRes=int(ds.attrs.get("MultiLookCrossRangeEffectiveResolution", 0)),
                                                 groundRes=int(ds.attrs.get("MultiLookGroundRangeEffectiveResolution", 0))),
        ds.attrs.get("L2Processor", ""),  # Only for L2
        ds.attrs.get("DopplerGMF", ""),  # Only for L2
        f"Kp{ds.attrs.get('Kp')}" if ds.attrs.get('Kp') else "",  # Only for L2
        f"RSV{ds.attrs.get('RSVNoise')}" if ds.attrs.get('RSVNoise') else "",  # Only for L2
        __version__,
    ]

    filename = "_".join(filter(None, filename_parts)) + ".nc"

    return filename
    
    
def check_attrs_dataset(ds):
    """
    Check dataset attributes.
    Test the dataset to check if all the attributes, from the defined list of mandatory atributes, are reported in the dataset.
    It test L1 and L2 datasets.

    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to check.

    Returns
    ----------
    ds : xr.Dataset
        The dataset.
    """
    
    logger.info("Checking the attrs dataset.")

    # Check if ds is a valid xarray Dataset
    if not isinstance(ds, xr.Dataset):
        logger.error("Input 'ds' must be an xarray.Dataset.")

    # Ensure processing_level is valid
    processing_level = ds.attrs.get("ProcessingLevel")
    valid_levels = {"L1AP", "L1B", "L1C", "L2"}
    if processing_level not in valid_levels:
        logger.error(f"Invalid processing level: {processing_level}. Must be one of {valid_levels}.")
        raise ValueError(f"Invalid processing level: {processing_level}. Must be one of {valid_levels}.")
    
    # List of the mandatory attributes for L1 dataset
    required_attrs = ["Campaign", 
                    "Platform", 
                    "Track", 
                    "StartTime", 
                    "EndTime", 
                    "ProcessingLevel", 
                    "Codebase", 
                    "Repository", 
                    "CodeVersion", 
                    "DataVersion", 
                    "Comments"]
    
    # Check missing attributes
    missing_attrs = [attr for attr in required_attrs if attr not in ds.attrs]
    
    # Extend the list of attributes for every dataset level
    if processing_level == 'L1B':
        required_attrs_L1B = ["MultiLookCrossRangeEffectiveResolution", "MultiLookGroundRangeEffectiveResolution"]
        missing_attrs.extend([attr for attr in required_attrs_L1B if attr not in ds.attrs])
    if processing_level == "L1C":
        required_attrs_L1C = ["Calibration", "NRCSGMF"]
        missing_attrs.extend([attr for attr in required_attrs_L1C if attr not in ds.attrs])
    if processing_level == "L2":
        required_attrs_L2 = ["DopplerGMF", "Kp", "RSVNoise", "L2Processor"]
        missing_attrs.extend([attr for attr in required_attrs_L2 if attr not in ds.attrs])

    # Raise error if missing attributes
    if missing_attrs:
        logger.error(f"The following attrs are missing in ds.attrs: {missing_attrs}")
    else:
        logger.info("All the attributes are reported in the dataset.")

    return ds

def clean_units_attribute(ds):
    """
    Clean units attributes from dataset.
    
    Looks for global and varible `units` attributes and removes all instances
    of square brackets around the `units` value.
    
    e.g., '[m]' --> 'm'
    
    Parameters
    ----------
    ds : ``xr.dataset``
        Input dataset to clean

    Returns
    -------
    ds : ``xr.dataset``
        Dataset with cleaned units attributes.
    """
    
    def remove_brackets(unit_str):
        return re.sub(r'[\[\]]', '', unit_str)
    
    
    # Check global attributes
    if 'units' in ds.attrs and isinstance(ds.attrs['units'], str):
        ds.attrs['units'] = remove_brackets(ds.attrs['units'])
    
    # Check each variable's attributes
    for var_name, var in ds.variables.items():
        if 'units' in var.attrs and isinstance(var.attrs['units'], str):
            var.attrs['units'] = remove_brackets(var.attrs['units'])
    
    return ds


def is_valid_acq_date(acq_date):
    """Check acquisition date.
    Check if the input acquisition date is a valid date string in 'YYYYMMDD' format.

    Parameters
    ----------
        acq_date (_type_): ``str``
            Acquisition date string to validate.

    Returns
    -------
        bool
            True if the date is valid and correctly formatted (YYYYMMDD), False otherwise.
    """
    try:
        dt.strptime(acq_date, "%Y%m%d")
        return bool(re.fullmatch(r"\d{8}", acq_date))
    except ValueError:
        return False

def is_valid_gmf_dict(gmf_dict):
    """
    Check the gmf dict in OSCAR L2 processing chain. Shall be:
    gmf={
        'nrcs': {'name': 'nscat4ds'},
        'doppler': {'name': 'mouche12' or 'yurovsky19'},
    }

    Parameters
    ----------
        gmf_dict : ``dict``
            Dictionnary containing the information about the NRCS GMF and Doppler GMF

    Returns:
        bool
            True if the format of the dictionnaire is correct.
    """
    # Clés et valeurs valides attendues
    valid_keys = {"nrcs", "doppler"}
    valid_values = {
        "nrcs": {"nscat4ds"},
        "doppler": {"mouche12", "yurovsky19"}
    }

    # Vérifie que toutes les clés sont valides
    for key in gmf_dict:
        if key not in valid_keys:
            logger.error(f"Invalid key: {key}. Expected keys: {valid_keys}. The code will crash.")
            raise ValueError(f"Invalid key: {key}. Expected keys: {valid_keys}")

        # Vérifie que la clé "name" existe pour chaque bloc
        if "name" not in gmf_dict[key]:
            logger.error(f"Missing 'name' field for key: {key}. The code will crash.")
            raise ValueError(f"Missing 'name' field for key: {key}")

        # Vérifie que la valeur est bien dans la liste autorisée
        value = gmf_dict[key]["name"]
        if value not in valid_values[key]:
            logger.error(f"Invalid value '{value}' for key '{key}'. Expected values: {valid_values[key]}. The code will crash.")
            raise ValueError(f"Invalid value '{value}' for key '{key}'. Expected values: {valid_values[key]}")

    return True

def find_file_by_track_name(files, track):
    """
    Find file by track name
    
    Matches a full OSCAR L1 or L2 file name by track name and returns the matched file name.
    `files` as generated using, e.g., `os.listdir('path_to_L1_data')`.

    Parameters
    ----------
    files : ``list`` of ``str``
        List of OSCAR L1 or L2 file names to search
    track : ``str``
        OSCAR Track name in the form, e.g., 'Track_1'

    Returns
    -------
    ``str``
        Full OSCAR file name matched with track name

    """
    
    escaped_track = re.escape(track)
    # Match the exact string followed by _ or . or end of string, but NOT more letters/numbers
    pattern = rf'{escaped_track}(?=(_|\.|$))'
    return [f for f in files if re.search(pattern, f)][0]

def coarsen_grid_resolution(ds, options):
    """
    Coarsen grid resolution.
    
    Coarsens an OSCAR dataset to new grid resolution specified in `options`.
    Dataset is coarsened to `MultiLookCrossRangeEffectiveResolution` x
    `MultiLookGroundRangeEffectiveResolution` in metres using the mean of
    variables data within this footprint.

    Parameters
    ----------
    ds : ``xr.Dataset``
        OSCAR L1B or L1C dataset containing the attrs `SingleLookCrossRangeGridResolution`
        and `SingleLookGroundRangeGridResolution`.
    options : ``dict``
        Options dict containing keys `MultiLookCrossRangeEffectiveResolution`
        and `MultiLookGroundRangeEffectiveResolution`.
    Raises
    ------
    Exception
        Raises an exception if not all required entries found in calib_dict

    Returns
    -------
    ds : ``xr.Dataset``
        OSCAR coarsened dataset.

    """
    
    valid_options = ['MultiLookCrossRangeEffectiveResolution', 'MultiLookGroundRangeEffectiveResolution']
    # Raise exception if both valid options not in options input
    if not all([option in options.keys() for option in valid_options]):
        raise Exception(str(valid_options) + ' not in options.')
    # Compute corsening intervals as final resolution / pixel resolution
    p_CrossRange = int(options.get('MultiLookCrossRangeEffectiveResolution') / ds.attrs.get('SingleLookCrossRangeGridResolution'))
    p_GroundRange = int(options.get('MultiLookGroundRangeEffectiveResolution') / ds.attrs.get('SingleLookGroundRangeGridResolution'))
    # Coarsen data using mean
    ds = ds.coarsen(GroundRange=p_GroundRange,boundary='trim').mean().coarsen(CrossRange=p_CrossRange,boundary='trim').mean()
    # Add attributes
    ds.attrs['SingleLookCrossRangeGridResolution'] = options.get('MultiLookCrossRangeEffectiveResolution')
    ds.attrs['SingleLookGroundRangeGridResolution'] = options.get('MultiLookGroundRangeEffectiveResolution')
    ds.attrs['MultiLookCrossRangeEffectiveResolution'] = options.get('MultiLookCrossRangeEffectiveResolution')
    ds.attrs['MultiLookGroundRangeEffectiveResolution'] = options.get('MultiLookGroundRangeEffectiveResolution')
    # Updating of the history in the attrs:
    current_history = ds.attrs.get("History", "")                                               # Get the current history or initialize it
    resolution_str = str(ds.attrs['SingleLookCrossRangeGridResolution']) + 'x' + str(ds.attrs['SingleLookGroundRangeGridResolution']) + 'm'
    new_entry = f"{dt.now(timezone.utc).strftime("%d-%b-%Y %H:%M:%S")} Coarsened grid resolution to " + resolution_str + '.'              # Create a new history entry
    updated_history = f"{current_history}\n{new_entry}" if current_history else new_entry           # Append to the history
    ds.attrs["History"] = updated_history                                                       # Update the dataset attributes
    
    return ds
