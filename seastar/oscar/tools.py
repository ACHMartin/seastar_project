# -*- coding: utf-8 -*-
"""Functions for the processing of OSCAR airbone data."""
import os
import importlib.util
import numpy as np
from seastar.utils.readers import readNetCDFFile
from seastar.utils.tools import list_duplicates
from scipy import interpolate
import xarray as xr
from datetime import datetime as dt

from _version import __version__


def load_OSCAR_data(file_path, file_inds):
    """
    Load OSCAR data.

    Loads in OSCAR Fore/Mid/Aft antenna files from the path designated in
    `file_path` and file indices in `file_inds`. Returns data in a dict of
    `{file index : xarray.Dataset}`

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR data files
    file_inds : ``list``
        List of indices of the Fore/Mid/Aft antenna data triplet within the
        file list generated by `os.listdir(file_path)`

    Returns
    -------
    ds : ``dict``, ``xarray.Dataset``
        OSCAR Fore/Mid/Aft antenna data as ``xarray.Dataset`` in a dict with
        keys as ``int`` to identify each antenna

    """
    file_list = sorted(os.listdir(file_path))
    ds = dict()
    for ind in file_inds:
        ds[ind] = readNetCDFFile(os.path.join(file_path, file_list[ind]))
    return ds


def identify_antenna_location(ds):
    """
    Identify ATI antenna location.

    Identifies the antenna direction in an OSCAR dataset by interrogating the
    minimum and maximum processed Doppler values.

    Parameters
    ----------
    ds : ``xarray.Dataset``
        OSCAR dataset containing MinProcessedDoppler and MaxProcessedDoppler
        variables

    Returns
    -------
    antenna_location : ``str``
        Antenna location ('Fore', 'Aft', 'Mid')

    """
    doppler_mean = np.mean([ds.MinProcessedDoppler, ds.MaxProcessedDoppler])
    if np.abs(doppler_mean) < 500:
        antenna_location = 'Mid'
    elif doppler_mean < 0:
        antenna_location = 'Aft'
    elif doppler_mean > 0:
        antenna_location = 'Fore'

    return antenna_location

def find_file_triplets(file_path):
    """
    Find file list indices of matching antenna files.

    Sorts a file list generated by `os.listdir(file_path)` and creates a list
    `file_time_triplets` of file aquisition times (YYYYMMDD'T'HHMMSS) along the
    1st column and the corresponding indices of the Fore/Mid/Aft antenna
    data files associated with each aquisition time in the 2nd column.

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR NetCDF files

    Returns
    -------
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list
        generated by `os.listdir(file_path)`
    """
    file_list = sorted(os.listdir(file_path))
    num_files = len(file_list)
    file_time = list()
    for file in range(num_files):
        file_info = str.split(file_list[file], '_')
        file_time.append(file_info[2])
    file_time_triplets = sorted(list_duplicates(file_time))
    return file_time_triplets


def antenna_idents(ds):
    """
    Build antenna identifier list.

    Generates list of antenna identifiers of the form ['Fore','Mid','Aft'] to
    correspond with keys in the dataset dictionary generated by
    `seastar.utils.readers.load_OSCAR_data`

    Parameters
    ----------
    ds : ``dict``
        OSCAR data stored as a dict with antenna number as keys and loaded
        data in ``xarray.Dataset`` format as values

    Returns
    -------
    antenna_ident : ``list``
        List of antenna identifiers in the form ['Fore', 'Mid', 'Aft'],
        corresponding to the data and keys stored in `ds`

    """
    antenna_id = list()
    for i in list(ds.keys()):
        antenna_id.append(identify_antenna_location(ds[i]))
    return antenna_id


def identify_antenna_location_from_filename(file_path, file_time_triplets):
    """
    Identify antenna from filename.

    Parameters
    ----------
    file_path : ``str``
        Local path to data files to check
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list.
        Generated by ``find_file_triplets``

    Returns
    -------
    antenna_id : ``list``
        List of antenna identifiers as strings like ['Mid', 'Fore', 'Aft']

    """
    antenna_identifiers = {'0': 'Mid', '3': 'Fore', '7': 'Aft'}
    file_list = sorted(os.listdir(file_path))
    antenna_id = list()
    for i in range(len(file_time_triplets)):
        file_name = file_list[file_time_triplets[i]]
        antenna_id.append(antenna_identifiers[file_name.split('_')[5][0]])
    return antenna_id


def colocate_variable_lat_lon(data_in, latitude, longitude, ds_out):
    """
    Co-locate data by lat/lon coordinates.

    Co-locates data from once set of lat/lon coordinates to another.

    Parameters
    ----------
    data_in : `xr.DataArray`, `array`
        Data at points to be co-locoated
    latitude : `array` of `float`
        Array of latitude coordinates
    longitude : `array` of `float`
        Array of longitude coordinates
    ds_out : `xr.DataArray`
        `xr.DataArray` with lat and lon coordinates to co-locate the data to.

    Returns
    -------
    colocated_var : `xr.DataArray`
        Data array of co-located data

    """
    new_data = interpolate.griddata(
                            points=(np.ravel(longitude),
                                    np.ravel(latitude)),
                            values=(np.ravel(data_in)),
                            xi=(ds_out.longitude.values,
                                ds_out.latitude.values),
                            )
    colocated_var = xr.DataArray(
                        data=new_data,
                        dims=ds_out.dims,
                        coords=ds_out.coords
                        )
    return colocated_var



def formatting_data(ds, processing_level, data_version, start_time, end_time, date_filename, resolution, track, platform="OSCAR", campaign="202305_MedSea"):
    """
    Formatting the attributes and the file name of the processed data from level 1B to level 2 included.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to format and to save as a NetCDF file.
    processing_level : `str`
        The processing level (e.g., "L1B", "L1C", "L2").
    version_file : `str`
        The path to the version file _version.py in the project.
    data_version : `str`
        The version of the MetaSensing data. Corresponding to the ftp deposite date.
    start_time : `str`
        The starting acquisition time.
    end_time : `str`
        The ending acquisition time.
    date_filename : `str`
        The init and final time of the acquisition with the date.
    resolution : `int`
        The pixel resolution of the OSCAR data.
    platform : `str, optional`
        The platform name (default is "OSCAR").       
    campaign : `str, optional`
        The campaign name (default is "202305_MedSea").  
            
    Returns
    ----------
    ds : xr.Dataset
        The dataset with updated metadata.
    filename : `str`
        Name of the OSCAR NetCDF file.
    """
    
    # Check if ds is a valid xarray Dataset
    if not isinstance(ds, xr.Dataset):
        raise TypeError("Input 'ds' must be an xarray.Dataset.")

    # Ensure processing_level is valid
    valid_levels = {"L1B", "L1C", "L2"}
    if processing_level not in valid_levels:
        raise ValueError(f"Invalid processing level: {processing_level}. Must be one of {valid_levels}.")
    
    
    # Building dataset
    if processing_level=="L1B" or processing_level=="L1C":
        ds.attrs['Campaign'] = campaign
        ds.attrs['Platform'] = platform
        ds.attrs['Track'] = track
        ds.attrs['StartTime'] = start_time
        ds.attrs['EndTime'] = end_time
        ds.attrs['ProcessingLevel'] = processing_level
        ds.attrs['Resolution'] = str(resolution).zfill(3)+"x"+str(resolution).zfill(3)+"m"
        ds.attrs['Codebase'] = 'seastar_project'
        ds.attrs['Repository'] = 'https://github.com/NOC-EO/seastar_project'
        ds.attrs['SoftwareVersion'] = __version__
        ds.attrs['DataVersion'] = data_version
        ds.attrs['Comments'] = 'Processed on ' + dt.today().strftime('%Y%m%d')
        
        filename = date_filename + "_" + ds.attrs['Platform'] + "_" + ds.attrs['ProductLevel'] + "_" + ds.attrs['Track'] + "_" + ds.attrs['Resolution'] + "_" + __version__ + ".nc"
        
    elif processing_level=="L2":
        ds.attrs['Kp'] = ds.Kp
        ds.attrs['RSV_noise'] = ds.RSVNoise
        ds.attrs['L2_Processor'] = ds.L2_processor  # Either PWP or FWC
        ds.attrs['GMF'] = ds.gmf                    # gmf for Doppler wind
        ds.attrs['Campaign'] = campaign
        ds.attrs['Platform'] = platform
        ds.attrs['Track'] = track
        ds.attrs['StartTime'] = start_time
        ds.attrs['EndTime'] = end_time
        ds.attrs['ProductLevel'] = processing_level # corresponding data level (L1B, L1C or L2)
        ds.attrs['Resolution'] = resolution
        ds.attrs['Codebase'] = 'seastar_project'
        ds.attrs['Repository'] = 'https://github.com/NOC-EO/seastar_project'
        ds.attrs['SoftwareVersion'] = __version__
        ds.attrs['DataVersion'] = data_version
        ds.attrs['Comments'] = 'Processed on ' + dt.today().strftime('%Y%m%d')
        
        filename = date_filename + "_" + ds.attrs['Platform'] + "_" + ds.attrs['ProductLevel'] + "_" + ds.attrs['Track'] + "_" + ds.attrs['Resolution'] + "_" + ds.attrs['L2_Processor'] + "_" + ds.attrs['GMF'] + "_Kp" + ds.attrs['Kp'] + "_RSV" + ds.attrs['RSV_noise'] + "_" + __version__ + ".nc"
        
    return ds, filename


def extract_acquisition_date(ds):
    """
    Extract the starting and ending datetime as well as the date and time acquisition to the good format to be reported in the filename of processed data.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       The L1ap dataset that contains date and time information.
    
    Returns
    ----------
    start_date : `str`
        The starting acquisition time.
    end_date : `str`
        The ending acquisition time.
    date_time_filename : `str`
        The date and time as it will appear in the post-processed data file name.
    """
    
    start_date = ds.Title.split()[2]
    end_date = ds.Title.split()[2].split("T")[0] + "T" + str(int(ds.FinalHour.data)).zfill(2)+str(int(ds.FinalMin.data)).zfill(2)+str(int(ds.FinalSec.data)).zfill(2)
    date_time_filename = ds.Title.split()[2] + "-" + str(int(ds.FinalHour.data)).zfill(2)+str(int(ds.FinalMin.data)).zfill(2)+str(int(ds.FinalSec.data)).zfill(2)
    
    
    return start_date, end_date, date_time_filename