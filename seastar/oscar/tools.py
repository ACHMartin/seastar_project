# -*- coding: utf-8 -*-
"""Functions for the processing of OSCAR airbone data."""
import os
import importlib.util
import numpy as np
from seastar.utils.readers import readNetCDFFile
from seastar.utils.tools import list_duplicates
from scipy import interpolate
import xarray as xr
from datetime import datetime as dt

from _version import __version__


def load_OSCAR_data(file_path, file_inds):
    """
    Load OSCAR data.

    Loads in OSCAR Fore/Mid/Aft antenna files from the path designated in
    `file_path` and file indices in `file_inds`. Returns data in a dict of
    `{file index : xarray.Dataset}`

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR data files
    file_inds : ``list``
        List of indices of the Fore/Mid/Aft antenna data triplet within the
        file list generated by `os.listdir(file_path)`

    Returns
    -------
    ds : ``dict``, ``xarray.Dataset``
        OSCAR Fore/Mid/Aft antenna data as ``xarray.Dataset`` in a dict with
        keys as ``int`` to identify each antenna

    """
    file_list = sorted(os.listdir(file_path))
    ds = dict()
    for ind in file_inds:
        ds[ind] = readNetCDFFile(os.path.join(file_path, file_list[ind]))
    return ds


def identify_antenna_location(ds):
    """
    Identify ATI antenna location.

    Identifies the antenna direction in an OSCAR dataset by interrogating the
    minimum and maximum processed Doppler values.

    Parameters
    ----------
    ds : ``xarray.Dataset``
        OSCAR dataset containing MinProcessedDoppler and MaxProcessedDoppler
        variables

    Returns
    -------
    antenna_location : ``str``
        Antenna location ('Fore', 'Aft', 'Mid')

    """
    doppler_mean = np.mean([ds.MinProcessedDoppler, ds.MaxProcessedDoppler])
    if np.abs(doppler_mean) < 500:
        antenna_location = 'Mid'
    elif doppler_mean < 0:
        antenna_location = 'Aft'
    elif doppler_mean > 0:
        antenna_location = 'Fore'

    return antenna_location

def find_file_triplets(file_path):
    """
    Find file list indices of matching antenna files.

    Sorts a file list generated by `os.listdir(file_path)` and creates a list
    `file_time_triplets` of file aquisition times (YYYYMMDD'T'HHMMSS) along the
    1st column and the corresponding indices of the Fore/Mid/Aft antenna
    data files associated with each aquisition time in the 2nd column.

    Parameters
    ----------
    file_path : ``str``
        Path containing OSCAR NetCDF files

    Returns
    -------
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list
        generated by `os.listdir(file_path)`
    """
    file_list = sorted(os.listdir(file_path))
    num_files = len(file_list)
    file_time = list()
    for file in range(num_files):
        file_info = str.split(file_list[file], '_')
        file_time.append(file_info[2])
    file_time_triplets = sorted(list_duplicates(file_time))
    return file_time_triplets


def antenna_idents(ds):
    """
    Build antenna identifier list.

    Generates list of antenna identifiers of the form ['Fore','Mid','Aft'] to
    correspond with keys in the dataset dictionary generated by
    `seastar.utils.readers.load_OSCAR_data`

    Parameters
    ----------
    ds : ``dict``
        OSCAR data stored as a dict with antenna number as keys and loaded
        data in ``xarray.Dataset`` format as values

    Returns
    -------
    antenna_ident : ``list``
        List of antenna identifiers in the form ['Fore', 'Mid', 'Aft'],
        corresponding to the data and keys stored in `ds`

    """
    antenna_id = list()
    for i in list(ds.keys()):
        antenna_id.append(identify_antenna_location(ds[i]))
    return antenna_id


def identify_antenna_location_from_filename(file_path, file_time_triplets):
    """
    Identify antenna from filename.

    Parameters
    ----------
    file_path : ``str``
        Local path to data files to check
    file_time_triplets : ``list``
        List of times of data aquisition and the corresponding indices of
        matching files in the Fore/Mid/Aft antenna triplet in the file list.
        Generated by ``find_file_triplets``

    Returns
    -------
    antenna_id : ``list``
        List of antenna identifiers as strings like ['Mid', 'Fore', 'Aft']

    """
    antenna_identifiers = {'0': 'Mid', '3': 'Fore', '7': 'Aft'}
    file_list = sorted(os.listdir(file_path))
    antenna_id = list()
    for i in range(len(file_time_triplets)):
        file_name = file_list[file_time_triplets[i]]
        antenna_id.append(antenna_identifiers[file_name.split('_')[5][0]])
    return antenna_id


def colocate_variable_lat_lon(data_in, latitude, longitude, ds_out):
    """
    Co-locate data by lat/lon coordinates.

    Co-locates data from once set of lat/lon coordinates to another.

    Parameters
    ----------
    data_in : `xr.DataArray`, `array`
        Data at points to be co-locoated
    latitude : `array` of `float`
        Array of latitude coordinates
    longitude : `array` of `float`
        Array of longitude coordinates
    ds_out : `xr.DataArray`
        `xr.DataArray` with lat and lon coordinates to co-locate the data to.

    Returns
    -------
    colocated_var : `xr.DataArray`
        Data array of co-located data

    """
    new_data = interpolate.griddata(
                            points=(np.ravel(longitude),
                                    np.ravel(latitude)),
                            values=(np.ravel(data_in)),
                            xi=(ds_out.longitude.values,
                                ds_out.latitude.values),
                            )
    colocated_var = xr.DataArray(
                        data=new_data,
                        dims=ds_out.dims,
                        coords=ds_out.coords
                        )
    return colocated_var



def formatting_data(
    ds, processing_level, data_version, start_time, end_time, date_filename, resolution, track, 
    L2_processor=None, Kp=None, RSVNoise=None, gmf =None, platform="OSCAR", campaign="202305_MedSea"):
    """
    Formatting the attributes and the file name of the processed data from level 1B to level 2 included.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       dataset to format and to save as a NetCDF file.
    processing_level : `str`
        The processing level (e.g., "L1B", "L1C", "L2").
    version_file : `str`
        The path to the version file _version.py in the project.
    data_version : `str`
        The version of the MetaSensing data. Corresponding to the ftp deposite date.
    start_time : `str`
        The starting acquisition time.
    end_time : `str`
        The ending acquisition time.
    date_filename : `str`
        The init and final time of the acquisition with the date.
    resolution : `int`
        The pixel resolution of the OSCAR data.
    track : `str`
        The Track number formatted as Track_01.
    L2_processor : `str`, optional
        Processor used for L2 processing. Can be PWP (PenWP) or FWC (Full Wind Current) (default is None.
    Kp : `float`, optional
        The Kp parameter, required for L2 (default is None.
    RSVNoise : `float`, optional
        The RSV noise parameter, optional for L2 (default is None.
    gmf : `str`, optional
        The Geophysical Model Function (GMF) name, required for L2. Can be mouche12 or yurovsky19 (default is None.
    platform : `str`, optional
        The platform name (default is "OSCAR").       
    campaign : `str`, optional
        The campaign name (default is "202305_MedSea").  
            
    Returns
    ----------
    ds : xr.Dataset
        The dataset with updated metadata.
    filename : `str`
        Name of the OSCAR NetCDF file.
    """
    
    # Check if ds is a valid xarray Dataset
    if not isinstance(ds, xr.Dataset):
        raise TypeError("Input 'ds' must be an xarray.Dataset.")

    # Ensure processing_level is valid
    valid_levels = {"L1B", "L1C", "L2"}
    if processing_level not in valid_levels:
        raise ValueError(f"Invalid processing level: {processing_level}. Must be one of {valid_levels}.")
    
    # Format resolution properly for metadata
    resolution_str = str(resolution).zfill(3)+"x"+str(resolution).zfill(3)+"m" 
    
    # Define general attributes
    metadata = {
        "Campaign": campaign,
        "Platform": platform,
        "Track": track,
        "StartTime": start_time,
        "EndTime": end_time,
        "ProcessingLevel": processing_level,
        "Resolution": resolution_str,
        "Codebase": "seastar_project",
        "Repository": "https://github.com/NOC-EO/seastar_project",
        "SoftwareVersion": __version__,
        "DataVersion": data_version,
        "Comments": f"Processed on {dt.today().strftime('%Y%m%d')}",
    }

    # Add L2-specific attributes
    if processing_level == "L2":
        
        # Ensure L2 processor is valid
        valid_L2_processor = {"PWP", "FCW"}
        if L2_processor not in valid_L2_processor:
            raise ValueError(f"Invalid L2 processor: {L2_processor}. Must be one of {valid_L2_processor}.")
        
        # Ensure GMF is valid
        valid_gmf = {"mouche12", "yurovsky19"}
        if gmf not in valid_gmf:
            raise ValueError(f"Invalid GMF: {gmf}. Must be one of {valid_gmf}.")
        
        # Ensure required L2 values are not None
        if any(var is None for var in [Kp, RSVNoise, L2_processor, gmf]):
            raise ValueError("Kp, RSVNoise, L2_processor, and gmf are required for processing level L2.")
    
        metadata.update({
            "Kp": Kp,
            "RSV_noise": RSVNoise,
            "L2_Processor": L2_processor,
            "GMF": gmf,
        })

        # Assign attributes to dataset
        ds.attrs.update(metadata)

   # Construct the filename
    filename_parts = [
        date_filename,  # Already formatted as per your logic
        platform,
        processing_level,
        track,
        resolution_str,
        L2_processor if L2_processor else "",  # Only for L2
        gmf if gmf else "",  # Only for L2
        f"Kp{Kp}" if Kp else "",  # Only for L2
        f"RSV{RSVNoise}" if RSVNoise else "",  # Only for L2
        __version__,
    ]

    filename = "_".join(filter(None, filename_parts)) + ".nc"


    return ds, filename


def extract_acquisition_date(ds):
    """
    Extract the starting and ending datetime as well as the date and time acquisition to the good format to be reported in the filename of processed data.
    
    Parameters
    ----------
    ds : `xr.DataArray`
       The L1ap dataset that contains date and time information.
    
    Returns
    ----------
    start_date : `str`
        The starting acquisition time.
    end_date : `str`
        The ending acquisition time.
    date_time_filename : `str`
        The date and time as it will appear in the post-processed data file name.
    """
    
    start_date = ds.Title.split()[2]
    end_date = ds.Title.split()[2].split("T")[0] + "T" + str(int(ds.FinalHour.data)).zfill(2)+str(int(ds.FinalMin.data)).zfill(2)+str(int(ds.FinalSec.data)).zfill(2)
    date_time_filename = ds.Title.split()[2] + "-" + str(int(ds.FinalHour.data)).zfill(2)+str(int(ds.FinalMin.data)).zfill(2)+str(int(ds.FinalSec.data)).zfill(2)
    
    
    return start_date, end_date, date_time_filename